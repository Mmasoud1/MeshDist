import sys
# list_recursive is a utility function to search recursively through a dictionary. 
# It's likely used to find specific keys within nested dictionary structures.
from ancillary import list_recursive
from dist import GenericLogger, training

class distributed():
    def initial_send_gradients(args):
        gradients = trainer.get_gradients()
        computation_output = {
            "output": {
                "gradients": gradients,     
                "computation_phase": 'local',
                "iterations": iterations,
                "current_iteration":current_iteration
            }
        }
        return computation_output
    
    def next_send_gradients(args):
        if args["input"]["last_iteration"]==False:
            trainer.agg_gradients(args["input"]["gradients"])
            gradients = trainer.get_gradients()
            computation_output = {
                "output": {
                    "gradients":gradients,       
                    "computation_phase": 'local',
                    "iterations": iterations,
                    "current_iteration":current_iteration
                }
            }
        else:
            trainer.save()
            computation_output = {"output":{"computation_phase":"end"}}
        return computation_output


def start(PARAM_DICT):  # The main entry point for local computation:
    # Checks if the computation_phase is defined in PARAM_DICT. 
    # If not, it initializes the training process.
    PHASE_KEY = list(list_recursive(PARAM_DICT, "computation_phase"))

    if not PHASE_KEY:
        global iterations, current_iteration, Dice, model, trainer, train_size
        # Get the base dir such as test/input/localx/simulatorRun/
        sys.path.append(PARAM_DICT['state']['baseDirectory'])
        # Imports necessary modules like meshnet, dice, and loader dynamically based on the path provided in PARAM_DICT.
        import meshnet, dice, loader
        current_iteration = 0
        #instance for logging.  /test/output/local0/simulatorRun/local.log
        logging = GenericLogger(PARAM_DICT['state']['outputDirectory']+'/local.log')
        logging.logger = logging._configure_logger()
        # Initializes the training instance with necessary parameters like database file, output path, learning rate, etc.
        trainer = training(databasefile='mindboggle.db',output_path=PARAM_DICT['state']['outputDirectory'], logger=logging,path = PARAM_DICT['state']['baseDirectory'],loader=loader, meshnet = meshnet, classes = PARAM_DICT['input']['classes'], learning_rate=PARAM_DICT['input']['lr'], Dice = dice)
        train_size = len(trainer.trainloader)
        # When the computation begins in the simulator, the first local site receives input generated by this 
        # line. This is in the form of an argument for the "command" listed in the computation specification, 
        # like so:
        # python computation/local.py {"input":{"epochs": 0, "lr" : 0.0007, "classes" : 3}, "cache": {}, state: ...}

        iterations = PARAM_DICT['input']['epochs']*train_size
        return distributed.initial_send_gradients(PARAM_DICT)
        
    else:
        # Depending on the phase (local or remote), it manages the flow of computation and gradient aggregation.
        if PARAM_DICT['input']['computation_phase']=='remote':
            current_iteration+=1
            trainer.current_iteration+=1
            return distributed.next_send_gradients(PARAM_DICT)
        else:
            raise ValueError("Error occurred at Local")

